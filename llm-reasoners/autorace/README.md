# AutoRace

This document will guide you to use the AutoRace to evaluate your reasoning chains (reproduce the experiments or apply it to your own datasets)

## Set the OpenAI_Key

```
export OPENAI_API_KEY= YOUR_OWN_OPEN_AI_KEY
```

## Reproduce the evaluation in paper

```
cd .../AutoRace
python autorace.py --dataset DATASET_NAME --prompt_type PROMPT_TYPE --model MODEL_TYPE
```

The `DATASET_NAME` is one of `dataset list` below:

`['gsm8k','strategyqa','AQuA','cosmos', 'multistep_arithmetic','word_sorting','logical_deduction']`.

The `MODEL_TYPE` is one of `model list` below:

`['dbrx','gpt-4-turbo','claude-3-opus','gemini-pro','internlm-2-7b','llama-2-70b','qwen-1.5-7b','gemma-7b','mistral-7b','llama-2-13b']`.

The `PROMPT_TYPE` can be found in `prompt.json`.

You can easily add your own datasets-model results under `data` directory and use `AutoRace_eval_dataset` function in `autorace.py` to get AutoRace Score. 

## Evaluate your own result

Theoretically, AutoRace can support any evaluation of Chain-of-Thought. In practice, you could first use the `AutoRace_criterion()` function in `AutoRace.py` to generate your own dataset's criterion prompt first and take `AutoRace_evaluation()` for evaluation. We showed an example from AQuA.

* **If you are using the dataset already available in `dataset list`, but testing a different model, you don't need to run `AutoRace_criterion()` to generate a criterion prompt.** You should use criterion prompt corresponding to the dataset in `prompt.json`.

## Dataset format

```jsonl
{
  "question": question in original dataset
  "metadata_generation": a rationale generated by LLM
  "gt_answer": the ground truth answer of the question in the dataset
}
```

